# DynamoDB-to-S3
**Step by step Breakdown:**
**Step 1: Set up the S3 bucket**  
- Go to the S3 console â†’ Create bucket â†’ Name it `dynamodb-csv-export-bucket` (or any unique name)  
- Leave all settings default â†’ Create bucket  

**Step 2: Create the DynamoDB table**  
- Go to DynamoDB console â†’ Create table  
- Table name: `UserData`  
- Partition key: `id` (String)  
- Add some sample data:  
```json
{
  "id": "1",
  "name": "Alice",
  "email": "alice@example.com"
}
```

**Step 3: Create the Lambda function**  
- Go to the Lambda console â†’ Create function  
- Name: `DynamoDBToCSV`  
- Runtime: Python 3.12  
- Role: Create a new role with basic permissions  

**Step 4: Add permissions to the Lambda role**  
- Go to IAM â†’ Roles â†’ Find your Lambda role  
- Attach policies:  
  - **AmazonDynamoDBReadOnlyAccess**  
  - **AmazonS3FullAccess**  

**Step 5: Write the Lambda function**  
Replace the default code with this:  
```
  lambda_func.py
```

**Step 6: Test the function**  
- In Lambda, click "Test" â†’ Create a new test event (empty payload)  
- Click "Test" again â€” you should see a success message  
- Go to your S3 bucket â†’ Check for the `dynamodb_data.csv` file ðŸŽ‰  

**Step 7: (Optional) Trigger from DynamoDB events**  
If you want the CSV to be updated every time data changes:  
- Go to DynamoDB â†’ Streams â†’ Enable stream on the table  
- Add a trigger in Lambda â†’ Choose the DynamoDB stream  
 
